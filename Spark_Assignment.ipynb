{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-4bc56731f3c8>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-4bc56731f3c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    294\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 296\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-4bc56731f3c8>:2 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.-Practice: Trending Topics & analysis sentiment (5.0%)\n",
    "\n",
    "<div class=activity>\n",
    "Implements the same application realized in activity 2 (mapreduce), but this time using Apache Spark:\n",
    "1. Calculate de the N Treding Topics (3.75%)\n",
    "    * Use the json twitter files.\n",
    "    * Clean-up and filter the input data\n",
    "    * Calculate the Trending topics (using the word-count and the Top-N models).\n",
    "    * (optional) Analysis sentiment of hashtgas (+1.0%)\n",
    "    * Store the N Trending topics in a output text file   \n",
    "2. Implement the standalone version of the previous Trending Topics application and study its performance (1.25%).\n",
    "    * Compare the performance with the mapreduce application.\n",
    "    * Performance vs Number of executors\n",
    "    * Performance vs Number of cores\n",
    "    * Scalability vs input data size\n",
    "    * Scalability vs number of executors/cores\n",
    "\n",
    "Deliver the jupyter notebook, the standalone application and a pdf file with the performance analysis. \n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the solution for \n",
    "\n",
    "<div class=exercise>\n",
    "\n",
    "#### 2. Input/output dataset exercise\n",
    "\n",
    "    Get all the spanish hastags topics and number of ocurrences using input json files and write then in a hadoop sequence file (you can use the textFile, filter,  saveAsNewAPIHadoopFile) \n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaner\n",
    "\n",
    "import os, shutil\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "if os.path.exists(\"Cleaned/Tweets_ES.json\"): \n",
    "    shutil.rmtree(\"Cleaned/Tweets_ES.json\")\n",
    "\n",
    "input = sc.textFile(\"Datasets/Tweets/tweets2.json\")\n",
    "tweets_lowercased = input.map(lambda x: json.loads(x.lower()))\n",
    "spanish_tweets = tweets_lowercased.filter(lambda t: \"es\" in t[\"lang\"])\n",
    "tweets_with_hashtags = spanish_tweets.filter(lambda t: t[\"entities\"][\"hashtags\"] != [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('con27estasparaelbailandomica', 1), ('soyc', 1), ('noticia', 1), ('valladolid', 1), ('fvf', 1), ('metgala', 1), ('pilarmode', 1), ('villegas', 1), ('valpo', 1), ('fotogalerIa', 1), ('tuitutil', 1), ('frasesdepelicula', 1), ('detox', 1), ('amlo', 1), ('osoriochong', 1), ('zavala', 1)]\n"
     ]
    }
   ],
   "source": [
    "#TrendingTopics\n",
    "\n",
    "topics = tweets_with_hashtags.flatMap(lambda t: map(lambda h: (unicodedata.normalize('NFKD', h[\"text\"]).encode('ascii','ignore'),1), t[\"entities\"][\"hashtags\"]))\\\n",
    ".reduceByKey(lambda a, b: a + b)\n",
    "print(topics.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TopNPattern\n",
    "\n",
    "sc.parallelize(topics.takeOrdered(5, lambda t: -t[1])).saveAsTextFile(\"Results/TopNPattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('osoriochong', 0.0), ('pilarmode', 0.0), ('zavala', 0.0), ('valladolid', -0.007352941176470588), ('soyc', 0.006944444444444444), ('valpo', 0.0070921985815602835), ('metgala', 0.0), ('fvf', 0.01639344262295082), ('amlo', 0.0), ('con27estasparaelbailandomica', 0.0), ('villegas', 0.0070921985815602835), ('detox', -0.010869565217391304), ('frasesdepelicula', 0.0), ('tuitutil', 0.0), ('fotogalerIa', 0.0), ('noticia', -0.007352941176470588)]\n"
     ]
    }
   ],
   "source": [
    "#HashtagSentiment\n",
    "\n",
    "def HashtagSentiment(tweet):\n",
    "    tweetLength = len(unicodedata.normalize('NFKD', tweet[\"text\"]).encode('ascii','ignore'))\n",
    "    tweetPolarity = 0.0\n",
    "    hashtags_with_polarity_and_length = []\n",
    "    \n",
    "    for word in unicodedata.normalize('NFKD', tweet[\"text\"]).encode('ascii','ignore').split(\" \"):\n",
    "        if len(word) > 0:\n",
    "            if word[0] == \"#\":\n",
    "                tweetPolarity += 0\n",
    "            elif(word in positive_words):\n",
    "                tweetPolarity += 1\n",
    "            elif(word in negative_words):\n",
    "                tweetPolarity -= 1\n",
    "            \n",
    "    for hashtag in tweet[\"entities\"][\"hashtags\"]:\n",
    "        hashtags_with_polarity_and_length.append(\n",
    "        (unicodedata.normalize('NFKD', hashtag[\"text\"]).encode('ascii','ignore'), tweetPolarity/tweetLength))\n",
    "    return hashtags_with_polarity_and_length\n",
    "\n",
    "positive_words = set(line.strip().lower() for line in open(\"Word_Classification/positive_words_es.txt\"))\n",
    "negative_words = set(line.strip().lower() for line in open(\"Word_Classification/negative_words_es.txt\"))\n",
    "\n",
    "sentiments_list = tweets_with_hashtags.map(lambda t: HashtagSentiment(t))\n",
    "flat_sentiments_list = [item for sublist in sentiments_list.collect() for item in sublist]\n",
    "\n",
    "print(sc.parallelize(flat_sentiments_list).reduceByKey(lambda a, b: a+b).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.- Practice: Calculate dataset statistics  (2.5%)\n",
    "<div class=activity>\n",
    "Using the Datasets/ml-100k/u.item data:\n",
    "1. Read the file and separate all the fields. [textFile, map, split, ...](0.5%)\n",
    "2. Calculate the following stastistics (0.5%):\n",
    "    * Number of Movies.  [count, ...]\n",
    "    * First record.  [first, ...]\n",
    "    * First 5 records.  [take, ...]\n",
    "3. Calculate Movies of year X (not take in consideration the blank years) [conver_year, map, filter, reduceByKey, *sortByKey*, collectAsMap, ...] (1.0%)\n",
    "    * Calculate the max, min, average and standard desviation of the movies by year. [max, min, map, mean,s tdev, ...]\n",
    "    * Plot a histogram with the number of movies by year (*optional*)    [np.arange, map, filter, distinct, plt.hist, plt.gcf,...]\n",
    "4. Working with partitions (0.5%)\n",
    "    * Get the Dataset's automatic number of partitions. [getNumPartitions, ...]\n",
    "    * Change the number of partitions to 10. [repartition, ...]\n",
    "\n",
    "The u.item file has information about the items (movies); this is a tab separated list with the following fields.\n",
    "              movie id | movie title | release date | video release date |\n",
    "              IMDb URL | unknown | Action | Adventure | Animation |\n",
    "              Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "              Thriller | War | Western |\n",
    "The last 19 fields are the genres, a 1 indicates the movie is of that genre, a 0 indicates it is not; movies can be in several generes at once. The movie ids are the ones used in the u.data data set.\n",
    "\n",
    "In [] there are the spark funtions required to implement the exercise. \n",
    "\n",
    "To get the movie year from the date and convert it to integer, you can use the following convert_year function: \n",
    "\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_year(x):\n",
    "     try:\n",
    "       return int(x[-4:])\n",
    "     except:\n",
    "       return 0 # there is a 'bad' data point with a blank year, which we set to 0 and will filter out late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.- Solution: Calculate dataset statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.- Results: Calculate dataset statistics\n",
    "\n",
    "Datasets/ml-100k/u.item MapPartitionsRDD[11] at textFile at NativeMethodAccessorImpl.java:-2\n",
    "\n",
    "1.- Movies fields:\n",
    "PythonRDD[12] at RDD at PythonRDD.scala:48\n",
    "()\n",
    "\n",
    "2.- Number of movies: 1682\n",
    "\n",
    "2.- First record: [u'1', u'Toy Story (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)', u'0', u'0', u'0', u'1', u'1', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0']\n",
    "\n",
    "2.- First 5 records:\n",
    "[[u'1', u'Toy Story (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)', u'0', u'0', u'0', u'1', u'1', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0'], [u'2', u'GoldenEye (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?GoldenEye%20(1995)', u'0', u'1', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0'], [u'3', u'Four Rooms (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0'], [u'4', u'Get Shorty (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)', u'0', u'1', u'0', u'0', u'0', u'1', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0'], [u'5', u'Copycat (1995)', u'01-Jan-1995', u'', u'http://us.imdb.com/M/title-exact?Copycat%20(1995)', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0']]\n",
    "\n",
    "3.- Movies by year histogram: \n",
    "[(1922, 1), (1926, 1), (1930, 1), (1931, 1), (1932, 1), (1933, 2), (1934, 4), (1935, 4), (1936, 2), (1937, 4), (1938, 3), (1939, 7), (1940, 8), (1941, 5), (1942, 2), (1943, 4), (1944, 5), (1945, 4), (1946, 5), (1947, 5), (1948, 3), (1949, 4), (1950, 7), (1951, 5), (1952, 3), (1953, 2), (1954, 7), (1955, 5), (1956, 4), (1957, 8), (1958, 9), (1959, 4), (1960, 5), (1961, 3), (1962, 5), (1963, 6), (1964, 2), (1965, 5), (1966, 2), (1967, 5), (1968, 6), (1969, 4), (1970, 3), (1971, 7), (1972, 3), (1973, 4), (1974, 8), (1975, 6), (1976, 5), (1977, 4), (1978, 4), (1979, 9), (1980, 8), (1981, 12), (1982, 13), (1983, 5), (1984, 8), (1985, 7), (1986, 15), (1987, 13), (1988, 11), (1989, 15), (1990, 24), (1991, 22), (1992, 37), (1993, 126), (1994, 214), (1995, 219), (1996, 355), (1997, 286), (1998, 65)]\n",
    "\n",
    "3.- Number of movies year 1994: 214\n",
    "\n",
    "3.- Maximun movies/year: (1996, 355)\n",
    "\n",
    "3.- Minimun movies/year: (1922, 1)\n",
    "\n",
    "3.- Average movies/year: 23.676056338\n",
    "\n",
    "3.- Standard desviation movies/year: 63.5534098658\n",
    "()\n",
    "\n",
    "4.- Num Partitions: 2\n",
    "\n",
    "4.- New Num Partitions: 10\n",
    "\n",
    "![Hist Image](imagenes/hist1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.- Practice: pair RDDs (3.0%)\n",
    "\n",
    "<div class=activity>\n",
    "Using the Datasets/ml-100k/u.data dataset:\n",
    "1. Read the ratings data and convert to numeric values (ids to integer and rating to float). [textFile, map, split, int(), float(), ...] (0.5%)\n",
    "2. Create a pair RDD with the following information: (user id, (rating, item id)) [map, ...] (0.5%)\n",
    "3. Calculate for each user the sum of the ratings and the number of movies rated. [aggregateByKey, ...] (0.5%)\n",
    "4. Calculate the average of ratings by movie for each user [mapValues, ...] (0.5%)\n",
    "5. Calculate how many ratings did each movie receive. [map, reduceByKey, ...](0.5%)\n",
    "6. High Rating Movies: How many movies had a higher than average (3) rating [map, filter, mapValues, reduceByKey, ...] (1.0%)\n",
    "    * Map the data to movie ID and rating.\n",
    "    * Filter the data only for those records with ratings 4 or higher.\n",
    "    * Map the data to movie ID and the number 1.\n",
    "    * Add each row of data together.\n",
    "7. Print the Top 5 the Last 5 rated movies [top, takeOrdered, sortBy,...]   (0.5%)\n",
    "8. Join the two movie_counts and high_rating_movies datasets using a leftOuterJoin [leftOuterJoin, tale, sortByKey, collect,...] (1.0%)\n",
    "    * Print the first 5 elements from the orignal and joining datasets. \n",
    "    * Print the data for the movid with id = 314.9\n",
    "9. Calculate the percent of ratings that are higher. [mapValues, top,...] (0.5%)\n",
    "    * Print the Top 10.\n",
    "\n",
    "You can choose the apartats you want to implement.\n",
    "\n",
    "The u.data file has full u data set, 100000 ratings by 943 users on 1682 items. \n",
    "* Each user has rated at least 20 movies.  \n",
    "* Users and items are numbered consecutively from 1.  \n",
    "* The data is randomly ordered. \n",
    "\n",
    "This is a tab separated list with the following fields.\n",
    "            \n",
    "\t         user id | item id | rating | timestamp. \n",
    "             \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 2.- Solution: pair RDDs (3.5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.- Result: pair RDDs (3.0%)\n",
    "\n",
    "1.- User ratings (user id, item id, rating, timestamp):[u'196', u'242', u'3', u'881250949']\n",
    "\n",
    "1.- Numerical user ratings (user id, rating, item id): (196, 3.0, 242)\n",
    "\n",
    "2.- Pair RDD (user id, (rating, item id)): (196, (3.0, 242))\n",
    "\n",
    "3.- Agregate user ratings and movies ((user id, (rating sum, number of movies)): \n",
    "[(2, (230.0, 62.0)), (4, (104.0, 24.0)), (6, (767.0, 211.0)), (8, (224.0, 59.0)), (10, (774.0, 184.0))]\n",
    "\n",
    "4.- User average ratings ((user id, rating sum/number of movies): \n",
    "[(2, 3.7096774193548385), (4, 4.333333333333333), (6, 3.6350710900473935), (8, 3.7966101694915255), (10, 4.206521739130435)]\n",
    "\n",
    "5.- Movie number of ratings (movie id, ratings number): \n",
    "[(2, 131), (4, 209), (6, 26), (8, 219), (10, 89)]\n",
    "\n",
    "6.- 1447 high rating movies (movie id, high ratings number): \n",
    "[(2, 51), (4, 122), (6, 15), (8, 155), (10, 59)]\n",
    "\n",
    "7.- Top 5 rating movies: \n",
    "[(50, 501), (100, 406), (181, 379), (127, 351), (174, 348)]\n",
    "[(50, 501), (100, 406), (181, 379), (127, 351), (174, 348)]\n",
    "[(50, 501), (100, 406), (181, 379), (127, 351), (174, 348)]\n",
    "7.- Last 5 rating movies: \n",
    "[(36, 1), (440, 1), (548, 1), (556, 1), (600, 1)]\n",
    "\n",
    "8.- Movie rating counts dataset (movie id, ratings number):     [(2, 131), (4, 209), (6, 26), (8, 219), (10, 89)]\n",
    "\n",
    "8.- High rating movies dataset (movie id, high ratings number): [(2, 51), (4, 122), (6, 15), (8, 155), (10, 59)]\n",
    "\n",
    "8.- Join (movie id, (ratings number, high ratings number):      [(2, (131, 51)), (4, (209, 122)), (6, (26, 15)), (8, (219, 155)), (10, (89, 59))]\n",
    "\n",
    "9.- Info for movie id 314: (314, (5, None))\n",
    "\n",
    "9.- Top higher rates movie (movie id, (high ratings number/ ratings number):\n",
    "[(814, 1.0), (1064, 1.0), (1080, 1.0), (1122, 1.0), (1130, 1.0), (1396, 1.0), (1398, 1.0), (1452, 1.0), (1458, 1.0), (1482, 1.0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.exercise {    \n",
       "    background-color:   #87f6d8 ;\n",
       "    border-color:  #02bdb6;\n",
       "    border-left: 5px solid  #02bdb6;\n",
       "    padding: 1.0em;\n",
       "    }\n",
       "div.activity {    \n",
       "    background-color:   #9af5fa   ;\n",
       "    border-color:  #027cbd ;\n",
       "    border-left: 8px solid  #027cbd ;\n",
       "    padding: 1.0em;\n",
       "    }\n",
       " </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/Exercise.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
